{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0d4482c-65aa-4da2-8296-3da145de84cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import math\n",
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk import edit_distance\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import PorterStemmer\n",
    "\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a65b03b8-462c-40bb-90b5-169588ebaf36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chat ID</th>\n",
       "      <th>Question</th>\n",
       "      <th>Success</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>163d0dcc5c3a4fd9b3d59715bbca781b</td>\n",
       "      <td>/id_a_registration</td>\n",
       "      <td>1</td>\n",
       "      <td>Aug 31, 2023, 11:51:02 PM</td>\n",
       "      <td>Greek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>163d0dcc5c3a4fd9b3d59715bbca781b</td>\n",
       "      <td>/greek_language</td>\n",
       "      <td>1</td>\n",
       "      <td>Aug 31, 2023, 11:50:55 PM</td>\n",
       "      <td>Greek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>163d0dcc5c3a4fd9b3d59715bbca781b</td>\n",
       "      <td>/initial_menu</td>\n",
       "      <td>1</td>\n",
       "      <td>Aug 31, 2023, 11:50:48 PM</td>\n",
       "      <td>User has not selected language yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>db3e652c039b45b4929dcc312f935bca</td>\n",
       "      <td>/initial_menu</td>\n",
       "      <td>1</td>\n",
       "      <td>Aug 31, 2023, 11:15:57 PM</td>\n",
       "      <td>User has not selected language yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4c26163d43a14033b52de2939929bfe5</td>\n",
       "      <td>/initial_menu</td>\n",
       "      <td>1</td>\n",
       "      <td>Aug 31, 2023, 11:07:47 PM</td>\n",
       "      <td>User has not selected language yet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0b1a17adb0e04717bb2c13b960a2b804</td>\n",
       "      <td>/id_b_redemption</td>\n",
       "      <td>1</td>\n",
       "      <td>Aug 31, 2023, 10:59:49 PM</td>\n",
       "      <td>Greek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0b1a17adb0e04717bb2c13b960a2b804</td>\n",
       "      <td>/menu</td>\n",
       "      <td>1</td>\n",
       "      <td>Aug 31, 2023, 10:59:15 PM</td>\n",
       "      <td>Greek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0b1a17adb0e04717bb2c13b960a2b804</td>\n",
       "      <td>Εγγραφή πόντων</td>\n",
       "      <td>1</td>\n",
       "      <td>Aug 31, 2023, 10:54:03 PM</td>\n",
       "      <td>Greek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0b1a17adb0e04717bb2c13b960a2b804</td>\n",
       "      <td>/id_c_benefits</td>\n",
       "      <td>1</td>\n",
       "      <td>Aug 31, 2023, 10:53:40 PM</td>\n",
       "      <td>Greek</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Chat ID            Question  Success  \\\n",
       "0  163d0dcc5c3a4fd9b3d59715bbca781b  /id_a_registration        1   \n",
       "1  163d0dcc5c3a4fd9b3d59715bbca781b     /greek_language        1   \n",
       "2  163d0dcc5c3a4fd9b3d59715bbca781b       /initial_menu        1   \n",
       "3  db3e652c039b45b4929dcc312f935bca       /initial_menu        1   \n",
       "4  4c26163d43a14033b52de2939929bfe5       /initial_menu        1   \n",
       "5  0b1a17adb0e04717bb2c13b960a2b804    /id_b_redemption        1   \n",
       "6  0b1a17adb0e04717bb2c13b960a2b804               /menu        1   \n",
       "7  0b1a17adb0e04717bb2c13b960a2b804     Εγγραφή πόντων         1   \n",
       "8  0b1a17adb0e04717bb2c13b960a2b804      /id_c_benefits        1   \n",
       "\n",
       "                   Timestamp                            Language  \n",
       "0  Aug 31, 2023, 11:51:02 PM                               Greek  \n",
       "1  Aug 31, 2023, 11:50:55 PM                               Greek  \n",
       "2  Aug 31, 2023, 11:50:48 PM  User has not selected language yet  \n",
       "3  Aug 31, 2023, 11:15:57 PM  User has not selected language yet  \n",
       "4  Aug 31, 2023, 11:07:47 PM  User has not selected language yet  \n",
       "5  Aug 31, 2023, 10:59:49 PM                               Greek  \n",
       "6  Aug 31, 2023, 10:59:15 PM                               Greek  \n",
       "7  Aug 31, 2023, 10:54:03 PM                               Greek  \n",
       "8  Aug 31, 2023, 10:53:40 PM                               Greek  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smile = pd.read_csv('SmartVision Smiles_Find Dialogue_Table.csv')\n",
    "smile.head(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f212e71-011a-4ca1-acc0-68509839b4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chat ID</th>\n",
       "      <th>Question</th>\n",
       "      <th>Success</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>401477e91cb84b9c9af5e4b049c74f21</td>\n",
       "      <td>Hello, I'm tring the registration procedure bu...</td>\n",
       "      <td>0</td>\n",
       "      <td>Aug 31, 2023, 8:47:01 PM</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0ea0ae2370f0434691ecdf024ca58c33</td>\n",
       "      <td>Οταν κΝω εγγραφή  και βαζω το επιθετο μου    δ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Aug 31, 2023, 5:41:50 PM</td>\n",
       "      <td>Greek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0ea0ae2370f0434691ecdf024ca58c33</td>\n",
       "      <td>Καλησπέρα  μια βοηθεια παρακαλω</td>\n",
       "      <td>0</td>\n",
       "      <td>Aug 31, 2023, 5:40:09 PM</td>\n",
       "      <td>Greek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>ea6493317a3b41c0bb96f2e97d35be2b</td>\n",
       "      <td>Έχω ξεχάσει την Καρτα μου κ θέλω να καταχωρήσω...</td>\n",
       "      <td>0</td>\n",
       "      <td>Aug 31, 2023, 2:50:09 PM</td>\n",
       "      <td>Greek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>ea6493317a3b41c0bb96f2e97d35be2b</td>\n",
       "      <td>Μας δουλεύεις μάλλον έχω γράψει 100 φορές των ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Aug 31, 2023, 2:48:28 PM</td>\n",
       "      <td>Greek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>c7fa826c24fa4befb59ba694e0d60566</td>\n",
       "      <td>Καλημέρα παρακαλώ δεν μπορώ να συνδεθώ στην κάρτα</td>\n",
       "      <td>0</td>\n",
       "      <td>Aug 31, 2023, 2:09:59 PM</td>\n",
       "      <td>Greek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>190d728f1d664aa7b31afb1067624c60</td>\n",
       "      <td>Που βρίσκεται το mysmiles</td>\n",
       "      <td>0</td>\n",
       "      <td>Aug 31, 2023, 10:19:04 AM</td>\n",
       "      <td>Greek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>432f2820fc9243c8a2927436dc5a3b6b</td>\n",
       "      <td>Pos pondus eko</td>\n",
       "      <td>0</td>\n",
       "      <td>Aug 30, 2023, 10:43:02 PM</td>\n",
       "      <td>Greek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>432f2820fc9243c8a2927436dc5a3b6b</td>\n",
       "      <td>ΤΗΕΛΟ ΝΑMATHO TUS PONDUS</td>\n",
       "      <td>0</td>\n",
       "      <td>Aug 30, 2023, 10:42:40 PM</td>\n",
       "      <td>Greek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>7d31e6973fdc44e3acd5b3a50b806dc5</td>\n",
       "      <td>Μπορείτε να μου εκδώσετε καινούργια;</td>\n",
       "      <td>0</td>\n",
       "      <td>Aug 30, 2023, 10:36:25 PM</td>\n",
       "      <td>Greek</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Chat ID  \\\n",
       "36   401477e91cb84b9c9af5e4b049c74f21   \n",
       "88   0ea0ae2370f0434691ecdf024ca58c33   \n",
       "90   0ea0ae2370f0434691ecdf024ca58c33   \n",
       "142  ea6493317a3b41c0bb96f2e97d35be2b   \n",
       "146  ea6493317a3b41c0bb96f2e97d35be2b   \n",
       "166  c7fa826c24fa4befb59ba694e0d60566   \n",
       "226  190d728f1d664aa7b31afb1067624c60   \n",
       "260  432f2820fc9243c8a2927436dc5a3b6b   \n",
       "261  432f2820fc9243c8a2927436dc5a3b6b   \n",
       "265  7d31e6973fdc44e3acd5b3a50b806dc5   \n",
       "\n",
       "                                              Question  Success  \\\n",
       "36   Hello, I'm tring the registration procedure bu...        0   \n",
       "88   Οταν κΝω εγγραφή  και βαζω το επιθετο μου    δ...        0   \n",
       "90                   Καλησπέρα  μια βοηθεια παρακαλω          0   \n",
       "142  Έχω ξεχάσει την Καρτα μου κ θέλω να καταχωρήσω...        0   \n",
       "146  Μας δουλεύεις μάλλον έχω γράψει 100 φορές των ...        0   \n",
       "166  Καλημέρα παρακαλώ δεν μπορώ να συνδεθώ στην κάρτα        0   \n",
       "226                          Που βρίσκεται το mysmiles        0   \n",
       "260                                     Pos pondus eko        0   \n",
       "261                           ΤΗΕΛΟ ΝΑMATHO TUS PONDUS        0   \n",
       "265               Μπορείτε να μου εκδώσετε καινούργια;        0   \n",
       "\n",
       "                     Timestamp Language  \n",
       "36    Aug 31, 2023, 8:47:01 PM  English  \n",
       "88    Aug 31, 2023, 5:41:50 PM    Greek  \n",
       "90    Aug 31, 2023, 5:40:09 PM    Greek  \n",
       "142   Aug 31, 2023, 2:50:09 PM    Greek  \n",
       "146   Aug 31, 2023, 2:48:28 PM    Greek  \n",
       "166   Aug 31, 2023, 2:09:59 PM    Greek  \n",
       "226  Aug 31, 2023, 10:19:04 AM    Greek  \n",
       "260  Aug 30, 2023, 10:43:02 PM    Greek  \n",
       "261  Aug 30, 2023, 10:42:40 PM    Greek  \n",
       "265  Aug 30, 2023, 10:36:25 PM    Greek  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus =  smile[smile.Success == 0]\n",
    "corpus.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c684c8f7-fba8-4b88-a4f5-34cb1afffdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stopword_and_stem_tokenizer(corpus):\n",
    "    \"\"\"\n",
    "    Custom tokenizer function to be passed in TfidfVectorizer.\n",
    "    Ignores english stopwords and applies stemming to remaining tokens.\n",
    "    \"\"\"\n",
    "    tokens = [word for sentence in nltk.word_tokenize(corpus) for word in nltk.word_tokenize(sentence)]\n",
    "    valid_tokens = []\n",
    "    for token in tokens:\n",
    "        valid_tokens.append(token)\n",
    "    result = [t for t in valid_tokens if t not in [stopwords.words('greek'),stopwords.words('english')]]\n",
    "    return result\n",
    "\n",
    "ss = np.array(corpus['Chat ID'])\n",
    "sm = np.array(corpus['Question'])\n",
    "si = np.array(corpus['Success'])\n",
    "sl = np.array(corpus['Timestamp'])\n",
    "se = np.array(corpus['Language'])\n",
    "stopwords_acc = string.punctuation\n",
    "\n",
    "\n",
    "corpora = []\n",
    "for i,sentence in enumerate(sm):\n",
    "    st = stopword_and_stem_tokenizer(sentence)\n",
    "    stl = str(st).replace(\"'\",\"\").replace('[','').replace(']','').replace(',','').replace('?','')\n",
    "    curr = word_tokenize(stl)\n",
    "    curr = [word for word in curr if word not in stopwords_acc]\n",
    "    corpora.append(curr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f82fb48-651d-470e-9ace-22623eba3425",
   "metadata": {},
   "source": [
    "$$ Pseudo \\ coded \\ class $$\n",
    "\n",
    "> Clustering Similar Data\n",
    "\n",
    "In an attempt to address fail response rates of a chat AI clustering new data on a self manifested ground truth was mandated. Moreover, regarrdless of any prior knowledge that stems from statisticl n-gram models the ground truth issue remained the same; it was impossible to use inductive reasoning. To further elaborate, in most cases smaller n-gram sequence was nontheless more thorough, yet it was also more vaguely stated. In addition, using longer n-gram sequences did not prove fruitful as such due to the fact that many re-occuring n-gram statements were sperated when on a retrospection they belonged to the same (n-1)-gram category. \n",
    "\n",
    "Following that line of reasoning, a better way to approach the failed responses issue was to cluster the data inductively. A ground truth was assembled only from the n-gram counts and the potential interctonections that should eventually result the formulation of larger n-grams. In simple words, for each n-gram a sequential search is implemented to determine the ground truth for (n+1)-grams; as such, there s a high potential that can be exploited that addresses unknown information to the current AI status or knonwn information that is confounded!\n",
    "\n",
    "Inspiration for this venture was mainly traced at nature as well as natural clusters; a natural cluster requires longer time to be assimilated than an artificial one. Although the notion of NLP and natural clusters is seemingly unrelated it was the very same patter that can be observed in this demonstration. Natural clusters were the entries where context of speech was not to be accounted for, while artificial cluster were the language formalities that an LM_AI could have hardly observed. \n",
    "\n",
    "As such, there is no cue as to whether an n_gram at any stage is a useful measure or not; the same reasoning applies for a list of n_grams that resulted from the same module. In the latter case, howerver, one thing can be attested; each artificially created list can be traced by that very module. Nonetheless, an aspiring idea that is, yet artificialy generated data that presumes uknown instances was to unrealistic and cumbersome to handle; the only reason that a simple n_gram was used to make a list of probable n_grams could be a generalization in the form of a ground truth. Ergo, if the alleged cluster cannot be reduced to the initial n_gram the new ground truth is a vaguely stated one.\n",
    "\n",
    "Unsurprisingly though, regardles of prior knowledge it could be asserted that deductions from smaller sequences apply to longer sequences. To be precise, a heuristic way to determine uknonw cluster data by trial and error. And that makes sense introspectively, as there is no basis for any n_gram to be discerned properly for its significance; another alternative is creating a ground truth from n_grams that is unique for each n_gram instance, which will be used as that basis, so it was manifested.\n",
    "\n",
    "\n",
    "__Note: the algorithm is a heuristic for clusters as there is no actual ground truth__\n",
    "\n",
    "> Inputs:\n",
    "    >\n",
    "    (i) conversations\n",
    "    (ii) threads\n",
    "    (iii) n-gram sequence size to begin \n",
    "    (iv) most common n-grams option\n",
    "    (v) length of ground truth tokens, in terms of words, n_gram separation during search\n",
    "    (vi) number of times for broader deductions n_gram --> n-1_gram\n",
    "\n",
    "> Process:\n",
    "    >\n",
    "    -first forms n_gram LM exists <E{(i),(ii),(iii),(iv)},\n",
    "    |-- then splits n_grams in LM <--> (iv)\n",
    "        --> searches for last to first element match of n-1_grams \n",
    "     >  \n",
    "    |-- forms n_gram LM (iv) \n",
    "        --> The process is excluding new instances until the clusters are formed and singular entities are removed\n",
    "            : (v) \n",
    "    >\n",
    "    |-- Compares LM models (v) from smaller sequences to longer\n",
    "        --> The process is excluding common instances until the final clusters are formed and the clusters are determined\n",
    "            : (vi) \n",
    "\n",
    "> Outputs:\n",
    "    >\n",
    "    : creates a ground truth of resulted clusters that is comprised of entries that survived mutual exclusion from more common instances or different phrasing on any FAQ clusters that resulted (vi). Eventually, it is a heuristic that potentially forms a pipeline ground truth; from that a result is guaranteed to be the most frequent n_gram at worst and at best the result is perfect.\n",
    "        --> Therefore, a search for these cluster entries can be done for resultant queries and provide the specifics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61aa96c9-c2a7-41f6-9619-c172364d9fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Heuristic_LM(object):\n",
    "    def __init__(self, data,texts, amp,tea,bredth=1,depth=2):\n",
    "        super(object, self).__init__()\n",
    "        self.data = data\n",
    "        self.texts = texts\n",
    "        self.amp = amp\n",
    "        self.t = depth\n",
    "        self.b = bredth\n",
    "        self.it = tea\n",
    "\n",
    "    def MCI(self,uk):\n",
    "        gb = list(k for k,_ in itertools.groupby(uk))\n",
    "        return gb\n",
    "\n",
    "    def __42__(self,ng,sup):\n",
    "        for i in ng:\n",
    "            print()\n",
    "            for j in i:\n",
    "                print(j)\n",
    "        return ng\n",
    "    \n",
    "    def _1001_(self,er):\n",
    "        st = []\n",
    "        for x,i in enumerate(er):\n",
    "            l = [item for sub_list in st for item in sub_list] #if re.search('[a-zA-Z]', item)] # ignore non-alphabetic characters]\n",
    "            d = []\n",
    "            for y in i:\n",
    "                if y not in l:\n",
    "                    d.append(y)\n",
    "            if d != [] : st.append(d)\n",
    "        return st\n",
    "        \n",
    "    def core(self,en):\n",
    "        red = self.b\n",
    "        l2 = []\n",
    "        for i in en:\n",
    "            og=ngrams(sequence=nltk.word_tokenize(i), n=red)\n",
    "            l = []\n",
    "            for grams in og:\n",
    "                l.append(' '.join(grams))\n",
    "            if l!=[]: l2.append(l)\n",
    "        return self._1001_(l2)\n",
    "    \n",
    "    def make_n_grams(self,data):\n",
    "        oui = []\n",
    "        n = self.amp\n",
    "        for i in range(1,n):\n",
    "            non = Counter()\n",
    "            for sent in data:\n",
    "                non.update(\n",
    "                    [gram for gram in ngrams(\n",
    "                        sent,\n",
    "                        i+1, \n",
    "                        pad_left=False, \n",
    "                        pad_right=False)\n",
    "                    ]\n",
    "                )\n",
    "            oui.append(non)\n",
    "        return oui\n",
    "    \n",
    "    def find(self,data):\n",
    "        csis = self.make_n_grams(data)\n",
    "        n = self.amp\n",
    "        clusters = []\n",
    "        try:\n",
    "            found = [' '.join(key) for key, _ in csis[n-2].most_common(self.it)]\n",
    "            clusters.append(found)\n",
    "        except:\n",
    "            print('Self Inclusion')\n",
    "        return clusters\n",
    "    \n",
    "    def cluster(self,data):\n",
    "        kyp = self.find(data)\n",
    "        meaning = self.core(self.find(data)[0])\n",
    "        return meaning\n",
    "\n",
    "    def GT(self,data):\n",
    "        times = self.t\n",
    "        truth = []\n",
    "        for i in range(times):\n",
    "            group = self.cluster(data)\n",
    "            groups = [i for i in group if len(i) >1]\n",
    "            self.__init__(conversations,threads,self.amp-i,100,self.b,times-1)\n",
    "            truth.append(groups)\n",
    "        return(self.MCI(truth))\n",
    "\n",
    "    def unrest(self,x,t=2):\n",
    "        dw = [i for i in self._1001_(x) if (len(i) > t and len(i)<3*t) ]\n",
    "        return self._1001_(dw)\n",
    "    \n",
    "    def FMA(self,data,hi=2):  \n",
    "        B = self.GT(data)\n",
    "        fe = []\n",
    "        for i in range(len(B)-1,-1,-1):\n",
    "            for j in B[i]:\n",
    "                fe.append(j)\n",
    "        if self.b == 1:\n",
    "            fi = self.unrest(self._1001_(fe),hi)\n",
    "        if self.b != 1:\n",
    "            print('Did n_gram search worth it?')\n",
    "            fi = self.__42__(self.unrest(self._1001_(fe),hi),self.b)\n",
    "        return fi\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca182a43-34ad-4cb1-9747-ad4edd7c34cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = [unidecode(i.lower()) for i in sm]\n",
    "conversations = [[unidecode(ch.lower()) for ch in word] for word in corpora]\n",
    "\n",
    "# CS_D(conversations,threads,n_gram_length_initialization,most_common_elements_count,ground_truth,n_gram_ground_truth,deduction_depth)\n",
    "\n",
    "ML = Heuristic_LM(conversations,threads,6,150,1,4)\n",
    "groups = ML.FMA(conversations,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f64ac13-02c4-4215-b277-443134b39cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['na kano eggraphe',\n",
       " 'ten karta mou',\n",
       " 'ton arithmo tes',\n",
       " '`` nt ``',\n",
       " 'blue star ferries',\n",
       " 'labo ekptose se',\n",
       " '210 89 19 800 epiloge',\n",
       " 'taxidepsei teleutaious 12',\n",
       " 'mpo ston logariasmo',\n",
       " 'like register for',\n",
       " 'thelo taxidepso pros eudelo ikarias',\n",
       " 'kliso eiseteria me alonou',\n",
       " 'eggrapho meso epharmoges',\n",
       " 'parakalo steilete te nea',\n",
       " 'kanei persi bazontas']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = [' '.join(i) for i in groups]\n",
    "queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7d73f8-693d-426f-8a9f-42eed49340fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a58acc-d4c4-4c31-9f27-6d276acee551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5735a083-0a94-4e61-81d8-e3a1cb70eb94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
